Code outline for basic GA neuroevolution:

Main function:
Takes parameters as inputs (pop size, mutation rates, fitness function? etc)
    Create N new genomes through some randomization
    Give fitness ranking to each in first gen
    For G generations run:
        rank all in current gen by fitness
        Make N-1 new genomes by randomly selecting one of T top parents, then mutating
        Take top 10 individuals from last gen:
            run fitness on each 30 times, take mean for new fitness
            take highest fitness of these, add to new gen
    Print last gen into file?
    return best genome


Issues to consider:
    Should tensors be float or double? Is there a significant cost to double?


Params to fiddle with:


1534 seconds for 1 gen on cuda, cpu use around 17-20% average, 960mb
1344 seconds for 1 gen on cpu, cpu use around 50%, 80mb

frostbite gpu:
11710 in evalFitness
776 in forward
3937 in atari step; 32 total (mostly waiting?)
ale_python_interface: ~3700 seconds, twice?

frostbite cpu:
4383 in evalFitness
342 in forward
3600 total in ale

venture cpu
13790 evalFitness
10800 ale


Notes:
Should list dependencies/installs in readme
pytorch torchvision, gym, gym[atari]

Ways to speed up:
Improve ALE use? (or fewer steps?)
Repeat tests to see if evalFitness itself is actually an issue; if so:
    try reworking action selection
    make env attached to genome? (no rebuilding each time)
    
    
More gpu times:

Frostbite:
1368 total, some in input/waiting
444 seconds in make? Might be calls to load ROM (502 calls, so yes)
1285/236 in evalFitness
443 ALE load ROM -only 502 calls; try to find/reduce these?
453 ALE step (act?)
19 on reset
236 + 443 + 453 + 19 = 1151 of evalFitness; also 66 in forward, only 68 unaccounted

Frostbite with envs created when genome is (only reset on evalFitness)
1456 total, some in waiting (less?)
1393/241.5 in evalFitness (slightly better?)
79 in forward
21 on reset
474 on ALE ROM
493 on ALE act

As above, but now envs copied from parent in mutation (maybe faster?)
928/242 in evalFitness
84 in forward
97 on ROM (no longer in eval)
493 on Act
23.5 on reset
242 + 84 + 97 + 493 + 23.5 = 876.5 in evalFitness, 51.5 unaccounted (find this- mayb just torch making/moving tensors?)
This is much faster and better; only concern is how much memory this takes up with a larger population (each env is kept in RAM the whole time now)
Not sure if I can improve ALE act at all... but where is the evalFitness bottleneck?!
options:
    use floats as torch default for everything here
    random action select to deterministic based on greatest value
    Could wrap each part of evalFitness in a sub-function, so cProfile points more directly to the issue

Next attempt:
5744/2026 in evalFitness??
447 on forward
2645 on Act
99 on ROM
24 on reset
seems pretty off

***ReLU before or after bias is added? Also, allow bias added to first layer?
Fix naming convetion for classes (Genome_network??)
!***Do I even need more than 1 env per thread? Do genomes need their own env?
Also, should the experiment carry the env var? makes things a little easier...


ssh poppelee@fries.reed.edu
scp -r mydir poppelee@fries.reed.edu:
nohup my process > results.txt
tmux

warning that scripts futurize and pasteurize are not installed on PATH (and convert-caffe2-to-onnx and vcvsa)--What was this error from again?

For NEAT, I am skipping the ability to re-enable genes; currently old genes are deleted. I don't think this will have a great impact; also it looks like re-enabling must be pretty conditional, as it doesn't make so much sense in the given examples (or maybe it does? page 12, 5->4 doesn't make much sense, I think...)

basic_evolve --> evolve_basic to keep with m.t. convention and make things clear

NEAT evolution is working, but highly favors basic architecture
Two things possibly going wrong: crossover not adding disjoint/excess correctly, or mutations screwing something up and making fitness much worse (affecting the network more than they should)
Test first with some crossover test, test second by observing tensors/model of mutated network

Thesis first/second chapter outline:
Abstract
Intro:
    Outline of problem/question
Intro to ML:
    basics
    neural nets
    traditional methods of training (SGD)
Intro to GAs:
    concept
    applications
Intro to Neuroevolutuion
    basics
    why its useful/applications
    quick history of innovations/algorithms
Lit review:
    various flavors of NEAT and their uses
    recent applications of non-NEAT algorithms
    other recent work?
    specific areas/problems: where NE seems better than SGD, where it's promising, where it falls short
    details on the uber paper
Statement of my problem:
    use uber paper as benchmark
    replicate code and recreate neat, apply to same problems
    after comparison, further investigation of more successful methods
    ideal: produce some variation/innovation that outperforms the uber results (or NEAT, if NEAT is better at the start)



NEAT:
how to control network size?
shrinking network operator?
disabling connections and nodes
run on XOR


Work on multithreading:
    I have figured out the issue with my previous attempts was that I was trying to pipe too much data across the whatnot
    So now I no longer am moving nets through the queue, since that broke the pipes
    Instead, nets are copied over only when a process is made, and then evalFitness is the only thing done on the thread
    The return value is just the fitness from the experiment, which is then assigned to the corresponding net in the main thread
    At first I made a thread for each net and did them all at once
    This was, unsurprisingly, a terrible idea
    Now I loop through the process several times with a maximum thread count
    CUDA might still not work. I am hoping it is functional as long as I don't try to pipe and CUDA-assigned memory around
    So I think the plan is then to keep all nets on CPU, then move them to GPU only in the evalFitness
    And hopefully moving to GPU from another thread is fine? But idk what to do if/when this fails

To add from NEAT paper:
    disable connections instead of removing
    explicit fitness sharing
    actual speciation (distinct species) rather than distance testing upon crossover
    use top percent of each species rather than top overall
    check how connections are re-enabled (longer paper)

Currently:
    Run a few times to fix bugs, then add the sharing
    then smooth out MT and clean code
    After that and some extensive testing/comparing results, I think this is ready for improvements/research?

replace ReLU with sigmoidal?

Note:
interspecies crossover currently not allowed (faster this way, and odds were only 0.001; will fix later?)
-------------------------

!! Find initial values and mutation effect? couldn't find in paper so borrowing from pytorch neat implementation


    Try XOR ASAP

    improve multithreading to whatever is fastest
Reorg:

    maybe restructure species as a subclass of population and inherit as much as possible


Important Issue:
    Paper's phrasing is somewhat ambiguous about whether disabled connections are counted in E and D or just in W
    (regarding the equation on page 13 (110) of the longer paper)
    at present I have this just implemented in W since that's my guess (and also less work)
    
Combining Population and Species:
    Identical:
        select
        sum fitness
        get item
        size
    pop similar but invokes species:
        add
        fit set
        reorder
        randOfSpecies/rand select
        check for improvement
    disjoint:
        assign offspring

Maybe rename weights in NEAT to connections? goes better with nodes and makes more sense

XOR worked once:
break through at gen 272, 15 at 370, continued to slowly gain until 500
!!!#------------------------#!!!
Experiment has concluded normally
Highest Fitness: 15.807660765013633
Press enter to continue to animation
Network output was: 0.0001022449650046387
Now checking on input: (0,1)
Network output was: 0.9952144977928111
Now checking on input: (1,0)
Network output was: 0.9919596231735874
Now checking on input: (1,1)
Network output was: 0.011186972610246293
Now checking on input: (0,0)
Fitness:
15.807660765013633
Nodes:
1 input 0 0
2 input 0 0
3 output 2 1.4096764446327414
314 hidden 1 -0.3874290058011651
Weights:
1 1 3 -4.1928734974729975
631 314 3 5.6608999882537265
630 2 314 1.3079756743815274
2 2 3 -3.851809327769589
685 1 314 2.631616349032996
Disabled Weights:

with:
Generation 499
Highest Fitness: 15.807660765013633
Total elapsed time:61.935682 secondsNumber of species: 29
Crossover
Mutating

Selecting Elite.
Best elite fitness is:  8.999330743570368
.
Best elite fitness is:  4.00000000265614
.
Best elite fitness is:  4.076899819573063
.
Best elite fitness is:  4.000000000002794
.
Best elite fitness is:  15.807660765013633
.
Best elite fitness is:  7.95777192013957
.
Best elite fitness is:  4.000587744408473
.
Best elite fitness is:  5.132619089414405
.
Best elite fitness is:  6.614049245311067
.
Best elite fitness is:  4.000628590136979
.
Best elite fitness is:  9.40838876106223
.
Best elite fitness is:  4.004154325162117
.
Best elite fitness is:  4.11267433689341
.
Best elite fitness is:  4.047907842000562

#---------------------------------------
Figures needed for thesis:
    Neuron and/or network

Change innovation nums to stop using all caps
standardize child and new to offspring
remove outfile system and just use prints?

assault, atlantis, seaquest -- GA worse than other methods, test to see if NEAT can succeed
frostbite, skiing, venture -- GA has best performance, test to see if i can replicate results and exceed them with tensor genome 



